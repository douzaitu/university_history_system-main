import os
import pandas as pd
import json
import re
from neo4j import GraphDatabase, exceptions
import ollama
import sys
from collections import defaultdict

# -------------------------- æ ¸å¿ƒé…ç½®ï¼ˆå¢å¼ºçº¦æŸï¼‰--------------------------
CONFIG = {
    "excel_path": "",
    "neo4j_uri": "bolt://localhost:7687",
    "neo4j_username": "neo4j",
    "neo4j_password": "12345678",
    "ollama_model": "qwen2:7b",  # æ›´ç¨³å®šçš„æ¨¡å‹
    "max_text_length": 500,
    "entity_types": [
        "æ•™å¸ˆå§“å", "é™¢ç³»", "èŒç§°", "ç ”ç©¶æ–¹å‘", "è¯¾ç¨‹åç§°", "æ¯•ä¸šé™¢æ ¡", "è£èª‰ç§°å·", "å·¥ä½œèŒè´£"
    ],
    "predefined_relations": [
        "å±äº", "æ‹¥æœ‰", "ç ”ç©¶", "ä¸»è®²", "æ¯•ä¸šäº", "è·å¾—", "è´Ÿè´£"
    ],
    "relation_mapping": {  # æ˜ç¡®å…³ç³»-å®ä½“ç±»å‹æ˜ å°„
        "å±äº": "é™¢ç³»",
        "æ‹¥æœ‰": "èŒç§°",
        "ç ”ç©¶": "ç ”ç©¶æ–¹å‘",
        "ä¸»è®²": "è¯¾ç¨‹åç§°",
        "æ¯•ä¸šäº": "æ¯•ä¸šé™¢æ ¡",
        "è·å¾—": "è£èª‰ç§°å·",
        "è´Ÿè´£": "å·¥ä½œèŒè´£"
    }
}


# -------------------------- 1. LLMå¢å¼ºçš„å®ä½“æå– --------------------------
def llm_enhance_entities(teacher_name, full_text):
    """ç”¨LLMä¿®æ­£/è¡¥å……å®ä½“æå–ç»“æœï¼Œå¤„ç†æ­£åˆ™æ— æ³•è¦†ç›–çš„å¤æ‚å¥å¼"""
    prompt = f"""
ä»…è¿”å›JSONï¼Œæ— å¤šä½™å†…å®¹ï¼
ä»»åŠ¡ï¼šä»æ–‡æœ¬ä¸­æå–{', '.join(CONFIG['entity_types'])}ï¼Œä¿®æ­£ä¸å®Œæ•´å®ä½“ï¼Œè¡¥å……é—æ¼å®ä½“ã€‚
æ³¨æ„ï¼šåŒ…å«"è´Ÿè´£"ã€"ä¸»æŒ"ã€"åˆ†ç®¡"ç­‰è¯çš„å†…å®¹å½’ç±»ä¸º"å·¥ä½œèŒè´£"ï¼Œä¸è¦å½’ç±»ä¸º"è£èª‰ç§°å·"ï¼
å·²çŸ¥å¯¼å¸ˆå§“åï¼š{teacher_name}
æ–‡æœ¬ï¼š{full_text[:CONFIG['max_text_length']]}
è¾“å‡ºæ ¼å¼ï¼š{{"æ•™å¸ˆå§“å": ["{teacher_name}"], "é™¢ç³»": [], "èŒç§°": [], "ç ”ç©¶æ–¹å‘": [], "è¯¾ç¨‹åç§°": [], "æ¯•ä¸šé™¢æ ¡": [], "è£èª‰ç§°å·": [], "å·¥ä½œèŒè´£": []}}
"""
    try:
        response = ollama.generate(
            model=CONFIG["ollama_model"],
            prompt=prompt,
            options={"temperature": 0.1, "max_tokens": 300}
        )
        # æå–å¹¶è§£æJSON
        json_str = re.search(r"\{.*\}", response["response"].strip(), re.DOTALL).group()
        llm_entities = json.loads(json_str)
        # è½¬æ¢ä¸º(å®ä½“, ç±»å‹)æ ¼å¼
        result = []
        for ent_type, ents in llm_entities.items():
            for ent in ents:
                if ent and ent.strip() and (ent.strip(), ent_type) not in result:
                    result.append((ent.strip(), ent_type))
        return result
    except Exception as e:
        print(f"LLMå®ä½“å¢å¼ºå¤±è´¥ï¼Œä½¿ç”¨åŸå§‹æå–ï¼š{e}")
        return extract_entities_from_text_v3(teacher_name, full_text)


def extract_entities_from_text_v3(teacher_name, full_text):
    """åŸå§‹å®ä½“æå–å‡½æ•°ï¼ˆä½œä¸ºLLMå¤±è´¥çš„å¤‡ç”¨ï¼‰"""
    entities = []
    # é™¢æ ¡è¡¥å…¨æ˜ å°„è¡¨ï¼ˆæ›´å…¨é¢ï¼‰
    school_mapping = {
        "æˆéƒ½ç†å·¥å¤§": "æˆéƒ½ç†å·¥å¤§å­¦",
        "å››å·å¤§": "å››å·å¤§å­¦",
        "ç”µå­ç§‘æŠ€å¤§": "ç”µå­ç§‘æŠ€å¤§å­¦",
        "æ—¥æœ¬ä¹å·å¤§": "æ—¥æœ¬ä¹å·å¤§å­¦",
        "è¥¿å—äº¤é€šå¤§": "è¥¿å—äº¤é€šå¤§å­¦",
        "æˆéƒ½ç†å·¥å­¦": "æˆéƒ½ç†å·¥å¤§å­¦",
        "æ¸…åå¤§": "æ¸…åå¤§å­¦",
        "åŒ—äº¬å¤§": "åŒ—äº¬å¤§å­¦",
        "å¤æ—¦å¤§": "å¤æ—¦å¤§å­¦"
    }

    # åº”ç”¨é™¢æ ¡è¡¥å…¨
    for short, full in school_mapping.items():
        full_text = full_text.replace(short, full)

    # 1. æ•™å¸ˆå§“åï¼ˆå¼ºåˆ¶æ·»åŠ ï¼‰
    if teacher_name and len(teacher_name.strip()) >= 2:
        entities.append((teacher_name.strip(), "æ•™å¸ˆå§“å"))

    # 2. èŒç§°ï¼ˆæ›´ç²¾å‡†çš„æ­£åˆ™ï¼‰
    title_pattern = r"(æ•™æˆ|å‰¯æ•™æˆ|è®²å¸ˆ|åŠ©æ•™|ç ”ç©¶å‘˜|å‰¯ç ”ç©¶å‘˜|é«˜çº§å®éªŒå¸ˆ|å·¥ç¨‹å¸ˆ)"
    title_matches = re.findall(title_pattern, full_text)
    for title in title_matches:
        if title and (title, "èŒç§°") not in entities:
            entities.append((title, "èŒç§°"))

    # 3. é™¢ç³»ï¼ˆæ›´ä¸¥æ ¼çš„åŒ¹é…ï¼‰
    dept_patterns = [
        r"([^ï¼Œã€‚ï¼›ï¼š\(\)ï¼ˆï¼‰]{2,10}[é™¢ç³»å­¦é™¢])",
        r"éš¶å±äº([^ï¼Œã€‚ï¼›ï¼š\(\)ï¼ˆï¼‰]{2,10}[é™¢ç³»å­¦é™¢])",
        r"ä¸»æŒ([^ï¼Œã€‚ï¼›ï¼š\(\)ï¼ˆï¼‰]{2,10}[é™¢ç³»å­¦é™¢])"
    ]
    for pattern in dept_patterns:
        dept_matches = re.findall(pattern, full_text)
        for match in dept_matches:
            dept = match if isinstance(match, str) else match[0]
            dept = dept.strip()
            if dept and len(dept) >= 4 and any(x in dept for x in ["é™¢", "ç³»"]) and (dept, "é™¢ç³»") not in entities:
                entities.append((dept, "é™¢ç³»"))

    # 4. å·¥ä½œèŒè´£ï¼ˆæ–°å¢ï¼‰
    work_patterns = [
        r"è´Ÿè´£([^ï¼Œã€‚ï¼›ï¼š\(\)ï¼ˆï¼‰]{5,80})",
        r"ä¸»æŒ([^ï¼Œã€‚ï¼›ï¼š\(\)ï¼ˆï¼‰]{5,80}å·¥ä½œ)",
        r"åˆ†ç®¡([^ï¼Œã€‚ï¼›ï¼š\(\)ï¼ˆï¼‰]{5,80})"
    ]
    for pattern in work_patterns:
        work_matches = re.findall(pattern, full_text)
        for work in work_matches:
            work = work.strip()
            if len(work) > 30:
                works = re.split(r"ã€|ï¼Œ", work)
                for w in works:
                    w = w.strip()
                    if w and len(w) >= 3 and len(w) <= 20 and (w, "å·¥ä½œèŒè´£") not in entities:
                        entities.append((w, "å·¥ä½œèŒè´£"))
            elif work and len(work) >= 3 and (work, "å·¥ä½œèŒè´£") not in entities:
                entities.append((work, "å·¥ä½œèŒè´£"))

    # 5. ç ”ç©¶æ–¹å‘ï¼ˆåˆ†å‰²å¤šä¸ªæ–¹å‘ï¼‰
    research_patterns = [
        r"ç ”ç©¶æ–¹å‘[ä¸ºï¼š:\s]*([^ï¼Œã€‚ï¼›ï¼š\(\)ï¼ˆï¼‰]{5,50})",
        r"ç ”ç©¶é¢†åŸŸ[ä¸ºï¼š:\s]*([^ï¼Œã€‚ï¼›ï¼š\(\)ï¼ˆï¼‰]{5,50})"
    ]
    for pattern in research_patterns:
        research_matches = re.findall(pattern, full_text)
        for research in research_matches:
            # åˆ†å‰²å¤šä¸ªç ”ç©¶æ–¹å‘
            directions = re.split(r"[,ï¼Œã€;ï¼›]", research)
            for direction in directions:
                direction = direction.strip()
                if direction and len(direction) >= 3 and (direction, "ç ”ç©¶æ–¹å‘") not in entities:
                    entities.append((direction, "ç ”ç©¶æ–¹å‘"))

    # 6. è¯¾ç¨‹åç§°
    course_patterns = [
        r"ä¸»è®²[ã€Š\s]*([^ã€‹ï¼Œã€‚ï¼›ï¼š]{3,20})[ã€‹]*è¯¾",
        r"ã€Š([^ã€‹]{3,20})ã€‹"
    ]
    for pattern in course_patterns:
        course_matches = re.findall(pattern, full_text)
        for course in course_matches:
            course = course.strip()
            if course and len(course) >= 3 and (course, "è¯¾ç¨‹åç§°") not in entities:
                entities.append((course, "è¯¾ç¨‹åç§°"))

    # 7. æ¯•ä¸šé™¢æ ¡
    school_patterns = [
        r"æ¯•ä¸šäº([^ï¼Œã€‚ï¼›ï¼š\(\)ï¼ˆï¼‰]{4,20}[å¤§å­¦å­¦é™¢ç ”ç©¶é™¢])",
        r"è·[ç¡•åš]å£«å­¦ä½äº([^ï¼Œã€‚ï¼›ï¼š\(\)ï¼ˆï¼‰]{4,20}[å¤§å­¦å­¦é™¢])"
    ]
    for pattern in school_patterns:
        school_matches = re.findall(pattern, full_text)
        for school in school_matches:
            school = school.strip()
            if school and any(x in school for x in ["å¤§å­¦", "å­¦é™¢", "ç ”ç©¶é™¢"]) and (school, "æ¯•ä¸šé™¢æ ¡") not in entities:
                entities.append((school, "æ¯•ä¸šé™¢æ ¡"))

    # 8. è£èª‰ç§°å·
    honor_patterns = [
        r"è·å¾—([^ï¼Œã€‚ï¼›ï¼š\(\)ï¼ˆï¼‰]{4,30}ç§°å·)",
        r"å…¥é€‰([^ï¼Œã€‚ï¼›ï¼š\(\)ï¼ˆï¼‰]{4,30}è®¡åˆ’)",
        r"([^ï¼Œã€‚ï¼›ï¼š\(\)ï¼ˆï¼‰]{4,30}äººæ‰)"
    ]
    for pattern in honor_patterns:
        honor_matches = re.findall(pattern, full_text)
        for honor in honor_matches:
            honor = honor.strip()
            if honor and len(honor) >= 4 and (honor, "è£èª‰ç§°å·") not in entities:
                entities.append((honor, "è£èª‰ç§°å·"))

    return entities


# -------------------------- 2. LLMè‡ªæˆ‘çº é”™çš„ä¸‰å…ƒç»„ç”Ÿæˆ --------------------------
def generate_relations_with_llm_correction(entities, text, teacher_name):
    """LLMç”Ÿæˆä¸‰å…ƒç»„+3è½®è‡ªæˆ‘çº é”™"""
    entity_dict = defaultdict(list)
    for ent, typ in entities:
        if ent and typ in CONFIG["entity_types"]:
            entity_dict[typ].append(ent)

    if not entity_dict.get("æ•™å¸ˆå§“å"):
        return []

    # åŸºç¡€æç¤ºè¯ï¼ˆå«æ ¼å¼çº¦æŸï¼‰
    base_prompt = f"""
ä»…è¿”å›JSONï¼ç”Ÿæˆä¸‰å…ƒç»„éœ€æ»¡è¶³ï¼š
1. subject={teacher_name}ï¼Œpredicateä»…ä»{CONFIG['predefined_relations']}é€‰æ‹©
2. å…³ç³»-å®ä½“æ˜ å°„ï¼š{json.dumps(CONFIG['relation_mapping'], ensure_ascii=False)}
3. objectä»å·²çŸ¥å®ä½“é€‰ï¼š{json.dumps(dict(entity_dict), ensure_ascii=False)}
ç‰¹åˆ«æ³¨æ„ï¼š"å·¥ä½œèŒè´£"ç±»å‹çš„å®ä½“å¯¹åº”"è´Ÿè´£"å…³ç³»ï¼
æ–‡æœ¬ï¼š{text[:300]}
è¾“å‡ºæ ¼å¼ï¼š{{"triples": [{{"subject": "{teacher_name}", "predicate": "", "object": ""}}]}}
"""

    # 3è½®é‡è¯•çº é”™
    for retry in range(3):
        try:
            response = ollama.generate(
                model=CONFIG["ollama_model"],
                prompt=base_prompt,
                options={"temperature": 0.0, "max_tokens": 400}
            )
            json_str = re.search(r"\{.*\}", response["response"].strip(), re.DOTALL).group()
            # åŸºç¡€æ ¼å¼ä¿®å¤
            json_str = json_str.replace("'", '"').replace(",\n}", "\n}").replace(", }", "}")
            triples = json.loads(json_str)["triples"]
            # éªŒè¯æœ‰æ•ˆæ€§
            valid = []
            for t in triples:
                subj = t.get("subject", "").strip()
                pred = t.get("predicate", "").strip()
                obj = t.get("object", "").strip()
                if (subj == teacher_name and
                        pred in CONFIG["predefined_relations"] and
                        obj and
                        CONFIG["relation_mapping"].get(pred) in entity_dict and
                        obj in entity_dict[CONFIG["relation_mapping"][pred]]):
                    valid.append((subj, pred, obj))
            if valid:
                return valid
            # çº é”™æç¤º
            base_prompt += f"\nä¸Šè½®é”™è¯¯ï¼šç”Ÿæˆçš„ä¸‰å…ƒç»„æ— æ•ˆï¼ˆå…³ç³»/å®ä½“ä¸åŒ¹é…ï¼‰ï¼Œè¯·é‡æ–°ç”Ÿæˆï¼"
        except Exception as e:
            base_prompt += f"\nä¸Šè½®é”™è¯¯ï¼š{str(e)[:50]}ï¼Œè¯·ä¿®æ­£JSONæ ¼å¼å¹¶é‡æ–°ç”Ÿæˆï¼"

    # å…œåº•ä½¿ç”¨è§„åˆ™å¼•æ“
    return generate_triples_by_rules(entity_dict, teacher_name)


# -------------------------- 3. è§„åˆ™å¼•æ“å¤‡ä»½ --------------------------
def generate_triples_by_rules(entity_dict, teacher_name):
    """ä½¿ç”¨è§„åˆ™å¼•æ“æ›¿ä»£Ollamaï¼Œç¡®ä¿ç”ŸæˆåŸºç¡€ä¸‰å…ƒç»„"""
    triples = []
    # æŒ‰å…³ç³»æ˜ å°„ç”Ÿæˆä¸‰å…ƒç»„
    for relation, entity_type in CONFIG["relation_mapping"].items():
        if entity_type in entity_dict and entity_dict[entity_type]:
            # å–ç¬¬ä¸€ä¸ªæœ‰æ•ˆå®ä½“
            entity_value = entity_dict[entity_type][0]
            triples.append((teacher_name, relation, entity_value))
    return triples


# -------------------------- 4. Excelè¯»å– --------------------------
def read_teacher_excel(excel_path):
    try:
        df = pd.read_excel(excel_path, engine="openpyxl")
    except Exception as e:
        raise ValueError(f"Excelè¯»å–å¤±è´¥ï¼š{str(e)}")

    excel_columns = df.columns.tolist()
    print(f"Excelè¯†åˆ«åˆ°çš„åˆ—åï¼š{excel_columns}")

    # å®šä½å§“åå’Œä¸ªäººä»‹ç»åˆ—
    name_col = next((col for col in excel_columns if any(key in str(col) for key in ["å§“å", "å¯¼å¸ˆå§“å"])), None)
    intro_col = next((col for col in excel_columns if any(key in str(col) for key in ["ä¸ªäººä»‹ç»", "è¯¦ç»†ä»‹ç»", "ç®€ä»‹"])),
                     None)
    if not name_col or not intro_col:
        raise ValueError("Exceléœ€åŒ…å«'å§“å'å’Œ'ä¸ªäººä»‹ç»'åˆ—")

    # ç”Ÿæˆç»“æ„åŒ–æ–‡æœ¬
    structured_texts = []
    df = df.fillna("")
    for idx, row in df.iterrows():
        teacher_name = str(row[name_col]).strip().replace(" ", "").replace("ã€€", "")
        personal_intro = str(row[intro_col]).strip()
        if not teacher_name:
            print(f"ç¬¬{idx + 1}è¡Œæ— å§“åï¼Œè·³è¿‡")
            continue
        full_text = f"å¯¼å¸ˆå§“åï¼š{teacher_name}ï¼›ä¸ªäººä»‹ç»ï¼š{personal_intro[:CONFIG['max_text_length']]}"
        full_text = re.sub(r"\d{4}å¹´|\dæœˆç”Ÿ|ç”·|å¥³|é‚®ç®±ï¼š.*?[ï¼Œã€‚]", "", full_text)
        structured_texts.append({
            "index": idx + 1,
            "teacher_name": teacher_name,
            "full_text": full_text
        })
        print(f"ç¬¬{idx + 1}è¡Œï¼šå§“å={teacher_name}ï¼Œæ–‡æœ¬é•¿åº¦={len(full_text)}å­—")

    print(f"\nExcelå¤„ç†å®Œæˆï¼šå…±{len(df)}è¡Œæ•°æ®ï¼Œç”Ÿæˆ{len(structured_texts)}æ¡æœ‰æ•ˆæ–‡æœ¬")
    return structured_texts


# -------------------------- 5. Neo4jæ“ä½œ --------------------------
class Neo4jGraphManager:
    def __init__(self, uri=CONFIG["neo4j_uri"], username=CONFIG["neo4j_username"], password=CONFIG["neo4j_password"]):
        try:
            self.driver = GraphDatabase.driver(uri, auth=(username, password))
            self.driver.verify_connectivity()
            print("Neo4jè¿æ¥æˆåŠŸï¼ˆå¯è§†åŒ–åœ°å€ï¼šhttp://localhost:7474ï¼‰")
        except exceptions.AuthError:
            raise ValueError("Neo4jç”¨æˆ·å/å¯†ç é”™è¯¯")
        except exceptions.ServiceUnavailable:
            raise ConnectionError("Neo4jæœåŠ¡æœªå¯åŠ¨")
        except Exception as e:
            raise Exception(f"Neo4jåˆå§‹åŒ–å¤±è´¥ï¼š{str(e)}")

    def close(self):
        if self.driver:
            self.driver.close()
            print("ğŸ”Œ Neo4jè¿æ¥å·²å…³é—­")

    def create_triple(self, head, relation, tail):
        try:
            with self.driver.session() as session:
                session.run("""
                    MERGE (h:Entity {name: $head})
                    MERGE (t:Entity {name: $tail})
                    MERGE (h)-[r:RELATION {type: $relation}]->(t)
                    RETURN h, r, t
                """, head=head, relation=relation, tail=tail)
            print(f"æ’å…¥ä¸‰å…ƒç»„ï¼š({head}, {relation}, {tail})")
        except Exception as e:
            print(f"æ’å…¥å¤±è´¥ï¼š({head}, {relation}, {tail})ï¼Œé”™è¯¯ï¼š{str(e)[:50]}")

    def batch_create_triples(self, triples):
        if not triples:
            print("ï¸æ— æœ‰æ•ˆä¸‰å…ƒç»„å¯å¯¼å…¥")
            return
        print(f"\n=== æ‰¹é‡å¯¼å…¥{len(triples)}ä¸ªä¸‰å…ƒç»„ ===")
        for triple in triples:
            self.create_triple(*triple)
        print(f"=== å¯¼å…¥å®Œæˆ ===")


# -------------------------- ä¸»æµç¨‹ --------------------------
def main():

    # ç¡®å®šExcelè·¯å¾„
    if len(sys.argv) > 1:
        CONFIG["excel_path"] = sys.argv[1]
    else:
        current_dir = os.getcwd()
        excel_files = [f for f in os.listdir(current_dir) if f.endswith((".xlsx", ".xls")) and "å¯¼å¸ˆ" in f]
        if excel_files:
            CONFIG["excel_path"] = os.path.join(current_dir, excel_files[0])
            print(f"è‡ªåŠ¨æ‰¾åˆ°Excelæ–‡ä»¶ï¼š{CONFIG['excel_path']}")
        else:
            print("æœªæ‰¾åˆ°å¯¼å¸ˆExcelæ–‡ä»¶")
            return

    # æ­¥éª¤1ï¼šè¯»å–Excel
    print("\nè¯»å–Excelæ•°æ®")
    try:
        structured_teachers = read_teacher_excel(CONFIG["excel_path"])
    except Exception as e:
        print(f"å¤±è´¥ï¼š{str(e)}")
        return

    # æ­¥éª¤2ï¼šæå–å®ä½“ï¼ˆLLMå¢å¼ºï¼‰
    print("\næ‰¹é‡æå–å®ä½“ï¼ˆLLMå¢å¼ºï¼‰")
    all_entities = []
    for teacher in structured_teachers:
        print(f"\nå¤„ç†ç¬¬{teacher['index']}ä½å¯¼å¸ˆï¼š{teacher['teacher_name']}")
        entities = llm_enhance_entities(teacher["teacher_name"], teacher["full_text"])
        print(f"æå–å®ä½“ï¼š{entities}")
        if entities:
            all_entities.append({
                "teacher_name": teacher["teacher_name"],
                "full_text": teacher["full_text"],
                "entities": entities
            })
    if not all_entities:
        print("æœªæå–åˆ°å®ä½“ï¼Œç»ˆæ­¢")
        return

    # æ­¥éª¤3ï¼šç”Ÿæˆä¸‰å…ƒç»„ï¼ˆLLMçº é”™ï¼‰
    print("\næ‰¹é‡ç”Ÿæˆä¸‰å…ƒç»„ï¼ˆLLMçº é”™ï¼‰")
    all_triples = []
    for teacher in all_entities:
        print(f"\nå¤„ç†å¯¼å¸ˆï¼š{teacher['teacher_name']}")
        triples = generate_relations_with_llm_correction(
            teacher["entities"], teacher["full_text"], teacher["teacher_name"]
        )
        if triples:
            all_triples.extend(triples)
            print(f"ğŸ”— æœ‰æ•ˆä¸‰å…ƒç»„ï¼š{triples}")
        else:
            print(f"æ— æœ‰æ•ˆä¸‰å…ƒç»„")

    # ä¸‰å…ƒç»„å»é‡
    unique_triples = list(set(tuple(t) for t in all_triples))
    print(f"\n=== ä¸‰å…ƒç»„æ±‡æ€» ===")
    print(f"åŸå§‹æ•°é‡ï¼š{len(all_triples)} | å»é‡åï¼š{len(unique_triples)}")

    # ç»Ÿè®¡å„å…³ç³»ç±»å‹æ•°é‡
    relation_count = {}
    for triple in unique_triples:
        rel = triple[1]
        relation_count[rel] = relation_count.get(rel, 0) + 1
    print(f"å…³ç³»ç±»å‹ç»Ÿè®¡ï¼š{relation_count}")

    # æ­¥éª¤4ï¼šå¯¼å…¥Neo4j
    print("\nå¯¼å…¥Neo4j")
    try:
        neo4j_manager = Neo4jGraphManager()
        neo4j_manager.batch_create_triples(unique_triples)
        neo4j_manager.close()
    except Exception as e:
        print(f"æ­¥éª¤4å¤±è´¥ï¼š{str(e)}")
        return

    print(f"è®¿é—® http://localhost:7474")
    print(f"æ‰§è¡ŒæŸ¥è¯¢ï¼šMATCH (h:Entity)-[r:RELATION]->(t:Entity) RETURN h, r, t LIMIT 50")


if __name__ == "__main__":
    # æ£€æŸ¥OllamaæœåŠ¡
    try:
        ollama.list()
    except Exception as e:
        print(f"OllamaæœåŠ¡æœªå¯åŠ¨ï¼Œå°†ä½¿ç”¨è§„åˆ™å¼•æ“æ¨¡å¼ï¼š{str(e)}")
    main()